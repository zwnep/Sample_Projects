{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e4571ef",
   "metadata": {},
   "source": [
    "# SOUND SIGNAL CLASSIFICATION USING DEEP LEARNING\n",
    "\n",
    "This project consists of 3 main steps:\n",
    "\n",
    "1. Step We will prepare our dataset for analysis and extract sound signal features from  audio files using Mel-Frequency Cepstral Coefficients(MFCC).\n",
    "\n",
    "2. Then we will build a Convolutional Neural Networks (CNN) model and train our model with our dataset. \n",
    "\n",
    "3. Finally we predict an audio file's class using our model.\n",
    "\n",
    "We will use UrbanSound8K Dataset, download Link is here: https://urbansounddataset.weebly.com/download-urbansound8k.html\n",
    "\n",
    "Dataset folder and this source code should be on same directory..\n",
    "\n",
    "Don't forget to install librosa library using anaconda promt with the following command line:\n",
    "\n",
    "conda install -c conda-forge librosa\n",
    "\n",
    "<IMG src=\"audiosignal.png\" width=\"500\" height=\"250\">\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d0f3c8",
   "metadata": {},
   "source": [
    "### Step 1: We will prepare our dataset for analysis and extract sound signal features from  audio files using Mel-Frequency Cepstral Coefficients(MFCC)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9180a909",
   "metadata": {},
   "source": [
    "Every signal has its own characteristics. In sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound. Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC.\n",
    "\n",
    "You can get detailed info about MFC on : https://www.youtube.com/watch?v=4_SH2nfbQZ8&t=0s\n",
    "\n",
    "So by using librosa library we will get characteristics of every audio signal in our dataset and hold them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d3b3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3c9183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f91f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I want to show to how librosa handles sound signals.\n",
    "# Let's read an example audio signal using librosa\n",
    "audio_file_path='UrbanSound8K/17973-2-0-32.wav'\n",
    "\n",
    "librosa_audio_data, librosa_sample_rate = librosa.load(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An important thing you should know about librosa is librosa converts any stereo signal into mono. \n",
    "# So librosa converted signal data is one dimensional since it converts all signals into mono and get \n",
    "# signal characteristics of your sound file over this mono signal form..\n",
    "\n",
    "print(librosa_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot the librosa audio data\n",
    "# Original audio with 1 channel \n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(librosa_audio_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60c213",
   "metadata": {},
   "source": [
    "Actually our sample is stereo, you can see it using scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770dda11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets read with scipy\n",
    "from scipy.io import wavfile as wav\n",
    "wave_sample_rate, wave_audio = wav.read(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd401b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e872ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original audio with 2 channels \n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(wave_audio)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d5b9f",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Here we will be using Mel-Frequency Cepstral Coefficients(MFCC) from the audio samples. The MFCC summarises the frequency distribution across the window size, so it is possible to analyse both the frequency and time characteristics of the sound. These audio representations will allow us to identify features for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = librosa.feature.mfcc(y=librosa_audio_data, sr=librosa_sample_rate, n_mfcc=40)   #n_mfcc: number of MFCCs to return \n",
    "print(mfccs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a35274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will extract MFCC's for every audio file in the dataset..\n",
    "\n",
    "audio_dataset_path='UrbanSound8K/audio/'\n",
    "metadata=pd.read_csv('UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efaa10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(filename):\n",
    "    audio, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
    "    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n",
    "    \n",
    "    return mfccs_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we iterate through every audio file and extract features \n",
    "# using Mel-Frequency Cepstral Coefficients\n",
    "extracted_features=[]\n",
    "for index_num,row in tqdm(metadata.iterrows()):\n",
    "    file_name = os.path.join(os.path.abspath(audio_dataset_path),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    final_class_labels=row[\"class\"]\n",
    "    data=features_extractor(file_name)\n",
    "    extracted_features.append([data,final_class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will convert extracted_features to Pandas dataframe\n",
    "extracted_features_df = pd.DataFrame(extracted_features,columns=['feature','class'])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f07897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then split the dataset into independent and dependent dataset\n",
    "X=np.array(extracted_features_df['feature'].tolist())\n",
    "y=np.array(extracted_features_df['class'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3e528",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd605ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b77c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should perform Label Encoding since we need one hot encoded values for output classes in our model (1s and 0s)\n",
    "\n",
    "# Please remember one-hot encoding:\n",
    "# 1 0 0 0 0 0 0 0 0 0 => air_conditioner\n",
    "# 0 1 0 0 0 0 0 0 0 0 => car_horn\n",
    "# 0 0 1 0 0 0 0 0 0 0 => children_playing\n",
    "# 0 0 0 1 0 0 0 0 0 0 => dog_bark\n",
    "# ...\n",
    "# 0 0 0 0 0 0 0 0 0 1 => street_music\n",
    "\n",
    "labelencoder=LabelEncoder()\n",
    "y=to_categorical(labelencoder.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e888a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5af694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split dataset as Train and Test\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb306c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08922fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e24947",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f90202",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041361f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f4be45",
   "metadata": {},
   "source": [
    "### Step 2: Building a Convolutional Neural Networks (CNN) Model and Train Our Model with UrbanSound8K Dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3211fd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many classes we have? We should  use it in ourm model\n",
    "num_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6121a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we start building our CNN model..\n",
    "\n",
    "model=Sequential()\n",
    "# 1. hidden layer\n",
    "model.add(Dense(125,input_shape=(40,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# 2. hidden layer\n",
    "model.add(Dense(250))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# 3. hidden layer\n",
    "model.add(Dense(125))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f3fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f68f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trianing the model\n",
    "\n",
    "epochscount = 300\n",
    "num_batch_size = 32\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=num_batch_size, epochs=epochscount, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f621887",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_test_set_accuracy = model.evaluate(X_test,y_test,verbose=0)\n",
    "print(validation_test_set_accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1834186",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c3225",
   "metadata": {},
   "source": [
    "### Step 3: Finally We Predict an Audio File's Class Using Our CNN Model.\n",
    "\n",
    "We first preprocess the new audio data and then predict the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dfe778",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"UrbanSound8K/PoliceSiren.wav\"\n",
    "sound_signal, sample_rate = librosa.load(filename, res_type='kaiser_fast') \n",
    "mfccs_features = librosa.feature.mfcc(y=sound_signal, sr=sample_rate, n_mfcc=40)\n",
    "mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mfccs_scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09074f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs_scaled_features = mfccs_scaled_features.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7699cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs_scaled_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5df793",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mfccs_scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ea1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mfccs_scaled_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66607a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array = model.predict(mfccs_scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_classes = [\"air_conditioner\",\"car_horn\",\"children_playing\",\"dog_bark\",\"drilling\", \"engine_idling\", \"gun_shot\", \"jackhammer\", \"siren\", \"street_music\"]\n",
    "\n",
    "result = np.argmax(result_array[0])\n",
    "print(result_classes[result]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e673a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9efe3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6688bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dda0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10a8549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa8ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5644131e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979f252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c6576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce28d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
